{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD \n",
    "This notebook is used to create the test suite for the SQuAD (Stanford Question Answering Dataset) task. It involves steps like selecting and preprocessing the data and defining test cases. For example, this test case will provide the context which meant some information, and author needs to provide question and exactly answer. and use model question-answering that has defined above,  `predconfs` function to provide and answer. and in this file has provided short summary of each test case. \n",
    "    \n",
    "    As an example of this, \n",
    "    ```\n",
    "    C: Laura became a waitress before Roy did.\n",
    "    Q: Who became a waitress last?\n",
    "    A: Roy\n",
    "    P: Laura\n",
    "    ```\n",
    "\n",
    "    Above is an example of fail test case, you can see that the context is \"Laura became a waitress `before` Roy did.\". The question is \"Who became a waitress `last`?\". The context and question are different is before and last, so the expected answer is \"Roy\", but the predicted answer from the model is \"Laura\". So mean that the question-answering model isn't understand the word before and after.\n",
    "    This task has tested on many capabilities.\n",
    "    \n",
    "    - Capability: Vocabulary\n",
    "    - Taxonomy: \n",
    "        - Size, shape, color, age, material\n",
    "        - Professions vs nationalities\n",
    "        - Animal vs vehicle\n",
    "    - Robustness\n",
    "    - NER\n",
    "    - Temporal\n",
    "    - Negation\n",
    "    - Fairness spinoff\n",
    "    - Coreference (Understanding which entities are referred to by “his / her”, “former / latter”)\n",
    "    - Semantic Role Labeling (SRL) (Understanding roles (Role and subject))\n",
    "\n",
    "    This notebook is responsible for creating test cases for SQuAD. After processing all test suites, it will save the test cases to squad_suite.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note:\n",
    "- MFT(Minimum Functionality Test): focuses on evaluating whether a model has the basic functionality \n",
    "- DIR(Directional Expectation test). determine whether a model’s predictions are consistent with a prior expectation or hypothesis \n",
    "- INV (Invariance testing) is a type of testing in ML that checks whether a model is invariant to certain transformations or changes in the input data. \n",
    "\n",
    "\n",
    "\n",
    "ref:\n",
    "- https://www.godeltech.com/how-to-automate-the-testing-process-for-machine-learning-systems/\n",
    "\n",
    "perturb is dataset evaluate the robustness(BERT)\n",
    "\n",
    "For the SQuAD use the dataset in the `datasets` library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "%reload_ext autoreload\n",
    "%autoreload 0\n",
    "\n",
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.expect import Expect\n",
    "from checklist.test_suite import TestSuite\n",
    "import numpy as np\n",
    "import spacy\n",
    "from checklist.perturb import Perturb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kafka\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.38112929463386536,\n",
       " 'start': 0,\n",
       " 'end': 19,\n",
       " 'answer': 'A new strain of flu'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, \\\n",
    "    AutoModelForQuestionAnswering, Trainer, TrainingArguments, HfArgumentParser\n",
    "from transformers import pipeline \n",
    "\n",
    "model = pipeline('question-answering')\n",
    "\n",
    "# This is just test the pipeline\n",
    "model({\n",
    "    'context': 'A new strain of flu that has the potential to become a pandemic has been identified by scientists.',\n",
    "    'question': 'What has been discovered by scientists?'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<checklist.text_generation.TextGenerator at 0x16e820f7090>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor = checklist.editor.Editor() # creates an instance of the Editor class \n",
    "editor.tg # generate the new text data of the Checklist library fro the testing model and suggest the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of question pair(inout) return two lists. to provide answer the test case\n",
    "def predconfs(context_question_pairs):\n",
    "    \"\"\"\n",
    "    output: predictions, confidence \n",
    "    source: https://github.com/marcotcr/checklist/blob/115f123de47ab015b2c3a6baebaffb40bab80c9f/notebooks/tutorials/5.%20Testing%20transformer%20pipelines.ipynb\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    confs = []\n",
    "    for c, q in context_question_pairs:\n",
    "        try:\n",
    "            p = model(question=q, context=c, truncation=True, )\n",
    "        except:\n",
    "            print('Failed', q)\n",
    "            preds.append(' ')\n",
    "            confs.append(1)\n",
    "        preds.append(p['answer'])\n",
    "        confs.append(p['score'])\n",
    "    return preds, np.array(confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to format the SQuAD context in the test case\n",
    "# e.g., (this is exmaple fail, just shown the format)\n",
    "# C: Claire is shorter than Donald.\n",
    "# Q: Who is shorter?\n",
    "# A: Claire (answer that answer)\n",
    "# P: Donald (predicted answer)\n",
    "def format_squad_with_context(x, pred, conf, label=None, *args, **kwargs):\n",
    "    c, q = x\n",
    "    ret = 'C: %s\\nQ: %s\\n' % (c, q)\n",
    "    if label is not None:\n",
    "        ret += 'A: %s\\n' % label\n",
    "    ret += 'P: %s\\n' % pred\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the SQuAD without context\n",
    "# e.g.,\n",
    "# Q: Where have the powers maintained peace in recent years?\n",
    "# P: United Nations\n",
    "def format_squad(x, pred, conf, label=None, *args, **kwargs):\n",
    "    c, q = x\n",
    "    ret = 'Q: %s\\n' % (q)\n",
    "    if label is not None:\n",
    "        ret += 'A: %s\\n' % label\n",
    "    ret += 'P: %s\\n' % pred\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test suite is container for the unit test. used for the test case.\n",
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kafka\\anaconda3\\Lib\\site-packages\\checklist\\text_generation.py:171: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  to_pred = torch.tensor(to_pred, device=self.device).to(torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smarter, better, older, younger, taller, worse, different, stronger, cooler, nicer, tougher, shorter, bigger, hotter, more, darker, happier, smaller, faster, richer, wiser, thinner, less, weaker, larger, quieter, cleaner, closer, healthier, heavier, colder, slower, harder, wealthier, safer, quicker, longer, higher, cheaper, thicker, louder, sharper, lighter, warmer, brighter, greater, deeper, lower, easier, softer, smoother, poorer, other, stranger, newer, stricter, simpler, clearer, superior, tighter\n"
     ]
    }
   ],
   "source": [
    "# suggest the mask from the context. In this context mask provided the adjective.\n",
    "print(', '.join(editor.suggest('{first_name} is {mask} than {first_name2}.')[:60]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define adjective\n",
    "adj = ['old', 'smart', 'tall', 'young', 'strong', 'short', 'tough', 'cool', 'fast', 'nice', 'small', 'dark', 'wise', 'rich', 'great', 'weak', 'high', 'slow', 'strange', 'clean']\n",
    "\n",
    "# append new column with same adjective\n",
    "adj = [(x.rstrip('e'), x) for x in adj]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tall', 'tall')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the test case\n",
    "# This test expect the answer will be first_name\n",
    "t = editor.template(\n",
    "    [(\n",
    "    '{first_name} is {adj[0]}er than {first_name1}.',\n",
    "    'Who is {adj[0]}er?'\n",
    "    )\n",
    "    ],\n",
    "    labels = ['{first_name}'],\n",
    "    adj=adj,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    save=True\n",
    "    )\n",
    "name = 'A is COMP than B. Who is more COMP?'\n",
    "description = ''\n",
    "test = MFT(**t, name=name, description=description, capability='Vocabulary')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 100 examples\n"
     ]
    }
   ],
   "source": [
    "# Run the example with 100 samples\n",
    "test.run(predconfs, n=100, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      498\n",
      "Test cases run:  100\n",
      "Fails (rate):    2 (2.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Claire is shorter than Donald.\n",
      "Q: Who is shorter?\n",
      "A: Claire\n",
      "P: Donald\n",
      "\n",
      "----\n",
      "C: Alison is greater than Ruth.\n",
      "Q: Who is greater?\n",
      "A: Alison\n",
      "P: Ruth\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# format the sample when summarizing the table\n",
    "test.summary(format_example_fn=format_squad_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This test case error because in the predconf does not understand what is the less\n",
    "t = editor.template(\n",
    "    [(\n",
    "    '{first_name} is {adj[0]}er than {first_name1}.',\n",
    "    'Who is less {adj[1]}?'\n",
    "    )\n",
    "    ],\n",
    "    labels = ['{first_name1}'], # label the right answer\n",
    "    adj=adj,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    save=True\n",
    "    )\n",
    "name = 'A is COMP than B. Who is less COMP?'\n",
    "description = ''\n",
    "test = MFT(**t, name=name, description=description, capability='Vocabulary')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 100 examples\n"
     ]
    }
   ],
   "source": [
    "# Run the example with 100 samples\n",
    "test.run(predconfs, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      497\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Martha is nicer than Linda.\n",
      "Q: Who is less nice?\n",
      "A: Linda\n",
      "P: Martha\n",
      "\n",
      "----\n",
      "C: Julie is younger than Joan.\n",
      "Q: Who is less young?\n",
      "A: Joan\n",
      "P: Julie\n",
      "\n",
      "----\n",
      "C: William is weaker than Lawrence.\n",
      "Q: Who is less weak?\n",
      "A: Lawrence\n",
      "P: William\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# format the sample when summarizing the table\n",
    "test.summary(format_example_fn=format_squad_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that contain the context and the qas\n",
    "# cross product between qas and context that contained associated labels\n",
    "def crossproduct(t):\n",
    "    # takes the output of editor.template and does the cross product of contexts and qas\n",
    "    ret = []\n",
    "    ret_labels = []\n",
    "    for x in t.data:\n",
    "        cs = x['contexts']\n",
    "        qas = x['qas']\n",
    "        d = list(itertools.product(cs, qas))\n",
    "        ret.append([(x[0], x[1][0]) for x in d])\n",
    "        ret_labels.append([x[1][1] for x in d])\n",
    "    t.data = ret\n",
    "    t.labels = ret_labels\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very, pretty, extremely, also, still, quite, more, really, not, clearly, fairly, incredibly, particularly, now, understandably, rather, cautiously, surprisingly, certainly, feeling, so, especially, definitely, generally, most, highly, super, reportedly, being, obviously\n"
     ]
    }
   ],
   "source": [
    "# show the suggestion words\n",
    "state = editor.suggest('John is very {mask} about the project.')[:20]\n",
    "print(', '.join(editor.suggest('John is {mask} {state} about the project.', state=state)[:30]))\n",
    "\n",
    "# define\n",
    "very = ['very', 'extremely', 'really', 'quite', 'incredibly', 'particularly', 'highly', 'super']\n",
    "somewhat = ['a little', 'somewhat', 'slightly', 'mildly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping use the crossproduct for the test case.\n",
    "# the crossproduct to map the context and qas along together\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} is {very} {s} about the project. {first_name1} is {s} about the project.',\n",
    "            '{first_name1} is {s} about the project. {first_name} is {very} {s} about the project.',\n",
    "            '{first_name} is {s} about the project. {first_name1} is {somewhat} {s} about the project.',\n",
    "            '{first_name1} is {somewhat} {s} about the project. {first_name} is {s} about the project.',\n",
    "            '{first_name} is {very} {s} about the project. {first_name1} is {somewhat} {s} about the project.',\n",
    "            '{first_name1} is {somewhat} {s} about the project. {first_name} is {very} {s} about the project.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who is most {s} about the project?',\n",
    "                '{first_name}'\n",
    "            ), \n",
    "            (\n",
    "                'Who is least {s} about the project?',\n",
    "                '{first_name1}'\n",
    "            ), \n",
    "            \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    s = state,\n",
    "    very=very,\n",
    "    somewhat=somewhat,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    save=True\n",
    "    ))\n",
    "name = 'Intensifiers (very, super, extremely) and reducers (somewhat, kinda, etc)?'\n",
    "desc = ''\n",
    "test = MFT(**t, name=name, description=desc, capability='Vocabulary')\n",
    "suite.add(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1200 examples\n",
      "Test cases:      499\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Catherine is a little bullish about the project. Adam is bullish about the project.\n",
      "Q: Who is least bullish about the project?\n",
      "A: Catherine\n",
      "P: Adam\n",
      "\n",
      "C: Catherine is a little bullish about the project. Adam is super bullish about the project.\n",
      "Q: Who is least bullish about the project?\n",
      "A: Catherine\n",
      "P: Adam\n",
      "\n",
      "C: Catherine is bullish about the project. Adam is super bullish about the project.\n",
      "Q: Who is least bullish about the project?\n",
      "A: Catherine\n",
      "P: Adam\n",
      "\n",
      "\n",
      "----\n",
      "C: Jim is upbeat about the project. Lucy is somewhat upbeat about the project.\n",
      "Q: Who is most upbeat about the project?\n",
      "A: Jim\n",
      "P: Lucy\n",
      "\n",
      "C: Jim is extremely upbeat about the project. Lucy is upbeat about the project.\n",
      "Q: Who is most upbeat about the project?\n",
      "A: Jim\n",
      "P: Lucy\n",
      "\n",
      "C: Jim is extremely upbeat about the project. Lucy is somewhat upbeat about the project.\n",
      "Q: Who is most upbeat about the project?\n",
      "A: Jim\n",
      "P: Lucy\n",
      "\n",
      "\n",
      "----\n",
      "C: Ellen is a little vocal about the project. Victoria is vocal about the project.\n",
      "Q: Who is least vocal about the project?\n",
      "A: Ellen\n",
      "P: Victoria\n",
      "\n",
      "C: Victoria is vocal about the project. Ellen is a little vocal about the project.\n",
      "Q: Who is least vocal about the project?\n",
      "A: Ellen\n",
      "P: Victoria\n",
      "\n",
      "C: Victoria is very vocal about the project. Ellen is vocal about the project.\n",
      "Q: Who is most vocal about the project?\n",
      "A: Victoria\n",
      "P: Ellen\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test.run(predconfs, n=100) # run the samples test case\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context) #summarize the test should include three samples. (assume that I do not know the result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size, chape, color, age, material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import munch\n",
    "# Initialize the data\n",
    "order = ['size', 'shape', 'age', 'color'] # assign name to use to append on the props. Accessing the list name in the properties\n",
    "props = []\n",
    "properties = {\n",
    "    'color' : ['red', 'blue','yellow', 'green', 'pink', 'white', 'black', 'orange', 'grey', 'purple', 'brown'],\n",
    "    'size' : ['big', 'small', 'tiny', 'enormous'],\n",
    "    'age' : ['old', 'new'],\n",
    "    'shape' : ['round', 'oval', 'square', 'triangular'],\n",
    "    'material' : ['iron', 'wooden', 'ceramic', 'glass', 'stone']\n",
    "}\n",
    "\n",
    "# Map the data append data into props\n",
    "for i in range(len(order)):\n",
    "    for j in range(i + 1, len(order)):\n",
    "        p1, p2 = order[i], order[j]\n",
    "        for v1, v2 in itertools.product(properties[p1], properties[p2]):\n",
    "            props.append(munch.Munch({\n",
    "                'p1': p1,\n",
    "                'p2': p2,\n",
    "                'v1': v1,\n",
    "                'v2': v2,\n",
    "            }))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sofa, couch, wall, carpet, chair, table, light, lamp, door, clock, mirror, desk, bed, TV, bar, television, window, box, tree, painting, curtain, fan, fridge, screen, wallpaper, piano, rug, shelf, camera, candle\n"
     ]
    }
   ],
   "source": [
    "# suggest word from the context\n",
    "print(', '.join(editor.suggest('There is {a:p.v1} {p.v2} {mask} in the room.', p=props, verbose=False)[:30]))\n",
    "\n",
    "# Assign the object\n",
    "objects = ['box', 'clock', 'table', 'object', 'toy', 'painting', 'sculpture', 'thing', 'figure']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: There is a painting in the room. The painting is oval and yellow.\n",
      "Q: What shape is the painting?\n",
      "A: oval\n",
      "P: oval and yellow\n",
      "\n",
      "C: There is an oval yellow painting in the room.\n",
      "Q: What shape is the painting?\n",
      "A: oval\n",
      "P: oval yellow\n",
      "\n",
      "C: There is a painting in the room. The painting is oval and yellow.\n",
      "Q: What color is the painting?\n",
      "A: yellow\n",
      "P: oval and yellow\n",
      "\n",
      "\n",
      "----\n",
      "C: There is a toy in the room. The toy is big and pink.\n",
      "Q: What size is the toy?\n",
      "A: big\n",
      "P: big and pink\n",
      "\n",
      "C: There is a big pink toy in the room.\n",
      "Q: What size is the toy?\n",
      "A: big\n",
      "P: pink\n",
      "\n",
      "\n",
      "----\n",
      "C: There is a clock in the room. The clock is tiny and orange.\n",
      "Q: What size is the clock?\n",
      "A: tiny\n",
      "P: tiny and orange\n",
      "\n",
      "C: There is a tiny orange clock in the room.\n",
      "Q: What size is the clock?\n",
      "A: tiny\n",
      "P: orange\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Test case on the taxonomy\n",
    "# Mapping use the crossproduct for the test case.\n",
    "# the crossproduct to map the context and qas along together\n",
    "# Map 2 qas into context. To check that if change the question the answer will answer correctly. The context contains all information.\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            'There is {a:p.v1} {p.v2} {obj} in the room.',\n",
    "            'There is {a:obj} in the room. The {obj} is {p.v1} and {p.v2}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'What {p.p1} is the {obj}?',\n",
    "                '{p.v1}'\n",
    "            ), \n",
    "            (\n",
    "                'What {p.p2} is the {obj}?',\n",
    "                '{p.v2}'\n",
    "            ), \n",
    "            \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    obj=objects,\n",
    "    p=props,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    save=True\n",
    "    ))\n",
    "name = 'size, shape, age, color'\n",
    "desc = ''\n",
    "test = MFT(**t, name=name, description=desc, capability='Taxonomy')\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Professions vs nationalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use `editor.suggest(...)` enerate suggestions based on the input string (first_name and profession which on a:mask).\n",
    "- {first_name} represents the suggestion name where a person's first name should be inserted.\n",
    "- {a:mask} represents a job title. `:mask` indicate a special words that provide the suggestion data for this holder.\n",
    "\n",
    "\n",
    "[:30] specifies that only the first 30 items from that list are provided.\n",
    "\n",
    "\n",
    "`editor.suggest('{first_name} {last_name} works as {a:mask}.')` is similar to the first, but this template also includes {last_name}. The suggestion generates both the first_name and last_name along with the profession. When adding the `last_name ` can createmore specific suggestion compared to the profession that contain only the `first_name`. Append this to the second list stored in prefessions. This is better that combbine into 1 list because it may cause the duplicate value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "professions = editor.suggest('{first_name} works as {a:mask}.')[:30]\n",
    "professions += editor.suggest('{first_name} {last_name} works as {a:mask}.')[:30]\n",
    "professions = list(set(professions)) # use set to remove duplicate the element, them convert back into list\n",
    "if 'translator' in professions:\n",
    "    professions.remove('translator') # remove this jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "def clean(string):\n",
    "    return string.lstrip('[a,the,an,in,at] ').rstrip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared clean function and predicted answer\n",
    "def expect_squad(x, pred, conf, label=None, meta=None):\n",
    "    return clean(pred) == clean(label)\n",
    "expect_squad = Expect.single(expect_squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    15 (15.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Emily is a Pakistani model.\n",
      "Q: What is Emily's job?\n",
      "A: model\n",
      "P: Pakistani model\n",
      "\n",
      "\n",
      "----\n",
      "C: Ken is a Russian model.\n",
      "Q: What is Ken's job?\n",
      "A: model\n",
      "P: Russian model\n",
      "\n",
      "\n",
      "----\n",
      "C: Philip is an Indian model.\n",
      "Q: What is Philip's job?\n",
      "A: model\n",
      "P: Indian model\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Test case on the profession and nationality\n",
    "# Mapping use the crossproduct for the test case.\n",
    "# the crossproduct to map the context and qas along together\n",
    "# the example of failed, the predicted answer provided occupation with nationality\n",
    "# to test that if change the question, jobs/nationality. The predicted answer can answer that which one is the nationality or profession.\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} is {a:nat} {prof}.',\n",
    "            '{first_name} is {a:prof}. {first_name} is {nat}.',\n",
    "            '{first_name} is {nat}. {first_name} is {a:prof}.',\n",
    "            '{first_name} is {nat} and {a:prof}.',\n",
    "            '{first_name} is {a:prof} and {nat}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'What is {first_name}\\'s job?',\n",
    "                '{prof}'\n",
    "            ), \n",
    "            (\n",
    "                'What is {first_name}\\'s nationality?',\n",
    "                '{nat}'\n",
    "            ), \n",
    "            \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    nat = editor.lexicons['nationality'][:10],\n",
    "    prof=professions,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    save=True,\n",
    "    ))\n",
    "name = 'Profession vs nationality'\n",
    "test = MFT(**t, name=name, expect=expect_squad, description='',  capability='Taxonomy')\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animal vs vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    47 (47.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Jeff has a hamster and a van.\n",
      "Q: What vehicle does Jeff have?\n",
      "A: van\n",
      "P: hamster and a van\n",
      "\n",
      "C: Jeff has a van and a hamster.\n",
      "Q: What vehicle does Jeff have?\n",
      "A: van\n",
      "P: van and a hamster\n",
      "\n",
      "\n",
      "----\n",
      "C: Claire has an iguana and a van.\n",
      "Q: What vehicle does Claire have?\n",
      "A: van\n",
      "P: iguana and a van\n",
      "\n",
      "\n",
      "----\n",
      "C: Benjamin has a rabbit and a car.\n",
      "Q: What vehicle does Benjamin have?\n",
      "A: car\n",
      "P: a rabbit and a car\n",
      "\n",
      "C: Benjamin has a car and a rabbit.\n",
      "Q: What vehicle does Benjamin have?\n",
      "A: car\n",
      "P: a car and a rabbit\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Test case on the animal and vehicle\n",
    "# Mapping use the crossproduct for the test case.\n",
    "# the crossproduct to map the context and qas along together\n",
    "# the example of failed, the predicted answer provided animal and vehicle\n",
    "# to test that if change the question, animal/vehicle. The predicted answer can answer that which one is the animal or vehicle or not.\n",
    "# The predicted answer does not understand the word `and` and the mask word that has been assigned to vehicle and animals\n",
    "animals = ['dog', 'cat', 'bull', 'cow', 'fish', 'serpent', 'snake', 'lizard', 'hamster', 'rabbit', 'guinea pig', 'iguana', 'duck']\n",
    "vehicles = ['car', 'truck', 'train', 'motorcycle', 'bike', 'firetruck', 'tractor', 'van', 'SUV', 'minivan']\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} has {a:animal} and {a:vehicle}.',\n",
    "            '{first_name} has {a:vehicle} and {a:animal}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'What animal does {first_name} have?',\n",
    "                '{animal}'\n",
    "            ), \n",
    "            (\n",
    "                'What vehicle does {first_name} have?',\n",
    "                '{vehicle}'\n",
    "            ), \n",
    "            \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    animal=animals,\n",
    "    vehicle=vehicles,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    save=True\n",
    "    ))\n",
    "name = 'Animal vs Vehicle'\n",
    "test = MFT(**t, name=name, description='', capability='Taxonomy', expect=expect_squad)\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    71 (71.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Betty bought a minivan. Kevin bought a snake.\n",
      "Q: Who bought an animal?\n",
      "A: Kevin\n",
      "P: Betty\n",
      "\n",
      "\n",
      "----\n",
      "C: Rose bought a duck. Lauren bought a tractor.\n",
      "Q: Who bought an animal?\n",
      "A: Rose\n",
      "P: Lauren\n",
      "\n",
      "C: Lauren bought a tractor. Rose bought a duck.\n",
      "Q: Who bought an animal?\n",
      "A: Rose\n",
      "P: Lauren\n",
      "\n",
      "\n",
      "----\n",
      "C: George bought a motorcycle. Kim bought a snake.\n",
      "Q: Who bought an animal?\n",
      "A: Kim\n",
      "P: George\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Test case on the animal and vehicle\n",
    "# Mapping use the crossproduct for the test case.\n",
    "# the crossproduct to map the context and qas along together\n",
    "# the example of failed, the predicted answer provided animal and vehicle\n",
    "# to test that if change the question, animal/vehicle. The predicted answer can answer the question that who bought the vehicle or animals or not\n",
    "\n",
    "animals = ['dog', 'cat', 'bull', 'cow', 'fish', 'serpent', 'snake', 'lizard', 'hamster', 'rabbit', 'guinea pig', 'iguana', 'duck']\n",
    "vehicles = ['car', 'truck', 'train', 'motorcycle', 'bike', 'firetruck', 'tractor', 'van', 'SUV', 'minivan']\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} bought {a:animal}. {first_name2} bought {a:vehicle}.',\n",
    "            '{first_name2} bought {a:vehicle}. {first_name} bought {a:animal}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who bought an animal?',\n",
    "                '{first_name}'\n",
    "            ), \n",
    "            (\n",
    "                'Who bought a vehicle?',\n",
    "                '{first_name2}'\n",
    "            ), \n",
    "            \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    animal=animals,\n",
    "    vehicle=vehicles,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    save=True\n",
    "    ))\n",
    "name = 'Animal vs Vehicle v2'\n",
    "test = MFT(**t, name=name, description='', capability='Taxonomy', expect=expect_squad)\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n",
      "Test cases:      443\n",
      "Test cases run:  100\n",
      "Fails (rate):    13 (13.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Chris is very outspoken. Diana is very modest.\n",
      "Q: Who is vocal?\n",
      "A: Chris\n",
      "P: Diana\n",
      "\n",
      "\n",
      "----\n",
      "C: Philip is very organized. Nick is very intelligent.\n",
      "Q: Who is smart?\n",
      "A: Nick\n",
      "P: Philip\n",
      "\n",
      "\n",
      "----\n",
      "C: Samuel is very happy. Tim is very humble.\n",
      "Q: Who is joyful?\n",
      "A: Samuel\n",
      "P: Tim\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Test case on the synnonyms\n",
    "# Mapping use the crossproduct for the test case.\n",
    "# the crossproduct to map the context and qas along together\n",
    "# This test case provides the normol context mainly test for synnonyms\n",
    "# In the context, provided that `Lisa` is very humble. `Jennie` is very thankful. The question will ask about who is modest? (ask with the synnonyms)\n",
    "# The expected answer is Lisa.\n",
    "\n",
    "synonyms = [ ('spiritual', 'religious'), ('angry', 'furious'), ('organized', 'organised'),\n",
    "            ('vocal', 'outspoken'), ('grateful', 'thankful'), ('intelligent', 'smart'),\n",
    "            ('humble', 'modest'), ('courageous', 'brave'), ('happy', 'joyful'), ('scared', 'frightened'),\n",
    "           ]\n",
    "\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} is very {s1[0]}. {first_name2} is very {s2[0]}.',\n",
    "            '{first_name2} is very {s2[0]}. {first_name} is very {s1[0]}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who is {s1[1]}?',\n",
    "                '{first_name}'\n",
    "            ), \n",
    "            (\n",
    "                'Who is {s2[1]}?',\n",
    "                '{first_name2}'\n",
    "            ), \n",
    "            \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    s=synonyms,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=250,\n",
    "    save=True\n",
    "   ))\n",
    "t += crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} is very {s1[1]}. {first_name2} is very {s2[1]}.',\n",
    "            '{first_name2} is very {s2[1]}. {first_name} is very {s1[1]}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who is {s1[0]}?',\n",
    "                '{first_name}'\n",
    "            ), \n",
    "            (\n",
    "                'Who is {s2[0]}?',\n",
    "                '{first_name2}'\n",
    "            ), \n",
    "            \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    s=synonyms,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=250,\n",
    "    save=True\n",
    "    )) \n",
    "name = 'Synonyms'\n",
    "test = MFT(**t, name=name, description='', capability='Taxonomy', expect=expect_squad)\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the opposite comparision words.\n",
    "comp_pairs = [('better', 'worse'), ('older', 'younger'), ('smarter', 'dumber'), ('taller', 'shorter'), ('bigger', 'smaller'), ('stronger', 'weaker'), ('faster', 'slower'), ('darker', 'lighter'), ('richer', 'poorer'), ('happier', 'sadder'), ('louder', 'quieter'), ('warmer', 'colder')]\n",
    "comp_pairs = list(set(comp_pairs))#list(set(comp_pairs + [(x[1], x[0]) for x in comp_pairs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n",
      "Test cases:      498\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Donald is taller than Judith.\n",
      "Q: Who is shorter?\n",
      "A: Judith\n",
      "P: Donald\n",
      "\n",
      "C: Judith is shorter than Donald.\n",
      "Q: Who is taller?\n",
      "A: Donald\n",
      "P: Judith\n",
      "\n",
      "\n",
      "----\n",
      "C: Alfred is colder than Jean.\n",
      "Q: Who is warmer?\n",
      "A: Jean\n",
      "P: Alfred\n",
      "\n",
      "C: Jean is warmer than Alfred.\n",
      "Q: Who is colder?\n",
      "A: Alfred\n",
      "P: Jean\n",
      "\n",
      "\n",
      "----\n",
      "C: Roger is worse than Elaine.\n",
      "Q: Who is better?\n",
      "A: Elaine\n",
      "P: Roger\n",
      "\n",
      "C: Elaine is better than Roger.\n",
      "Q: Who is worse?\n",
      "A: Roger\n",
      "P: Elaine\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Test case on the taxonomy comparision\n",
    "# Mapping use the crossproduct for the test case.\n",
    "# the crossproduct to map the context and qas along together\n",
    "# In the context, it provide the comparision words, but the question ask the opposite word of that comparision word which to test the taxonomy.\n",
    "\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} is {comp[0]} than {first_name1}.',\n",
    "            '{first_name1} is {comp[1]} than {first_name}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who is {comp[1]}?',\n",
    "                '{first_name1}',\n",
    "            ),\n",
    "            (\n",
    "                'Who is {comp[0]}?',\n",
    "                '{first_name}',\n",
    "            )\n",
    "            \n",
    "        ]\n",
    "        ,\n",
    "    },\n",
    "    comp=comp_pairs,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    save=True\n",
    "    ))\n",
    "name = 'A is COMP than B. Who is antonym(COMP)? B'\n",
    "test = MFT(**t, name=name, description='', capability='Taxonomy')\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1600 examples\n",
      "Test cases:      498\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Mary is more hopeful than Julie.\n",
      "Q: Who is less hopeful?\n",
      "A: Julie\n",
      "P: Mary\n",
      "\n",
      "C: Julie is more hopeless than Mary.\n",
      "Q: Who is less hopeless?\n",
      "A: Mary\n",
      "P: Julie\n",
      "\n",
      "C: Mary is less hopeless than Julie.\n",
      "Q: Who is more hopeless?\n",
      "A: Julie\n",
      "P: Mary\n",
      "\n",
      "\n",
      "----\n",
      "C: Andrew is more positive than Katherine.\n",
      "Q: Who is less positive?\n",
      "A: Katherine\n",
      "P: Andrew\n",
      "\n",
      "C: Andrew is more positive than Katherine.\n",
      "Q: Who is more negative?\n",
      "A: Katherine\n",
      "P: Andrew\n",
      "\n",
      "C: Katherine is more negative than Andrew.\n",
      "Q: Who is less negative?\n",
      "A: Andrew\n",
      "P: Katherine\n",
      "\n",
      "\n",
      "----\n",
      "C: Mark is less conservative than Sophie.\n",
      "Q: Who is more conservative?\n",
      "A: Sophie\n",
      "P: Mark\n",
      "\n",
      "C: Mark is more progressive than Sophie.\n",
      "Q: Who is less progressive?\n",
      "A: Sophie\n",
      "P: Mark\n",
      "\n",
      "C: Mark is more progressive than Sophie.\n",
      "Q: Who is more conservative?\n",
      "A: Sophie\n",
      "P: Mark\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Test case on the antonym, Test that the test case can be tested about more/less. Is it effectively?\n",
    "# Mapping use the crossproduct for the test case.\n",
    "# the crossproduct to map the context and qas along together\n",
    "\n",
    "# in the context, it provided the context like `Mary is more hopeful than Julie.`. The question will ask `Q: Who is less hopeful?`. \n",
    "# The expected answer should be Julie, but the answer is Mary. (Falied test case) \n",
    "\n",
    "\n",
    "antonym_adjs = [('progressive', 'conservative'),('religious', 'secular'),('positive', 'negative'),('defensive', 'offensive'),('rude',  'polite'),('optimistic', 'pessimistic'),('stupid', 'smart'),('negative', 'positive'),('unhappy', 'happy'),('active', 'passive'),('impatient', 'patient'),('powerless', 'powerful'),('visible', 'invisible'),('fat', 'thin'),('bad', 'good'),('cautious', 'brave'), ('hopeful', 'hopeless'),('insecure', 'secure'),('humble', 'proud'),('passive', 'active'),('dependent', 'independent'),('pessimistic', 'optimistic'),('irresponsible', 'responsible'),('courageous', 'fearful')]\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} is more {a[0]} than {first_name1}.',\n",
    "            '{first_name1} is more {a[1]} than {first_name}.',\n",
    "            '{first_name} is less {a[1]} than {first_name1}.',\n",
    "            '{first_name1} is less {a[0]} than {first_name}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who is more {a[0]}?',\n",
    "                '{first_name}',\n",
    "            ),\n",
    "            (\n",
    "                'Who is less {a[0]}?',\n",
    "                '{first_name1}',\n",
    "            ),\n",
    "            (\n",
    "                'Who is more {a[1]}?',\n",
    "                '{first_name1}',\n",
    "            ),\n",
    "            (\n",
    "                'Who is less {a[1]}?',\n",
    "                '{first_name}',\n",
    "            ),\n",
    "        ]\n",
    "        ,\n",
    "    },\n",
    "    a = antonym_adjs,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    save=True\n",
    "    ))\n",
    "name = 'A is more X than B. Who is more antonym(X)? B. Who is less X? B. Who is more X? A. Who is less antonym(X)? A.'\n",
    "test = MFT(**t, name=name, description='', capability='Taxonomy')\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------NOT USED--------------------------------------------------------\n",
    "# original code from https://github.com/marcotcr/checklist/blob/115f123de47ab015b2c3a6baebaffb40bab80c9f/notebooks/SQuAD.ipynb\n",
    "\n",
    "# import pickle\n",
    "# data, answers =  load_squad()\n",
    "# spacy_map =  pickle.load(open('/home/marcotcr/tmp/processed_squad.pkl', 'rb'))\n",
    "# pairs = [(x['passage'], x['question']) for x in data]\n",
    "# processed_pairs = [(spacy_map[x[0]], spacy_map[x[1]]) for x in pairs]\n",
    "#--------------------------------------------------------------------------------\n",
    "# Ps. I tried to find the dataset, but i didnot find, so i decide to install the datasets library to used the 'squad' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\kafka\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\kafka\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kafka\\appdata\\roaming\\python\\python311\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\kafka\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kafka\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "# from datasets import load_dataset\n",
    "dataset = datasets.load_dataset('squad')\n",
    "pairs = [(x['context'], x['question']) for x in dataset['train']] # train is in the squad dataset library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/marcotcr/checklist/blob/115f123de47ab015b2c3a6baebaffb40bab80c9f/notebooks/QQP.ipynb\n",
    "# all_questions = list(all_questions)\n",
    "# parsed_questions = list(nlp.pipe(all_questions))\n",
    "# spacy_map = dict([(x, y) for x, y in zip(all_questions, parsed_questions)])\n",
    "\n",
    "# python -m spacy download en_core_web_sm (install)\n",
    "\n",
    "# This is the model from Spacy library used for the NLP task below in the parsed_question. \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "all_questions = set() # a set of all questions and context\n",
    "\n",
    "# add data in dataset to all_question\n",
    "for x in dataset['train']:\n",
    "    all_questions.add(x['question'])\n",
    "    all_questions.add(x['context'])\n",
    "\n",
    "#turn question into pipeline then convert to list\n",
    "parsed_questions = list(nlp.pipe(all_questions)) \n",
    "\n",
    "# map the original question and question that turned into list\n",
    "spacy_map = dict([(x, y) for x, y in zip(all_questions, parsed_questions)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_pairs = [(spacy_map[x[0]], spacy_map[x[1]]) for x in pairs] # process the pair question using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Catholic,\n",
       " the Main Building's,\n",
       " the Virgin Mary.,\n",
       " the Main Building,\n",
       " Venite Ad Me Omnes,\n",
       " the Main Building,\n",
       " the Sacred Heart,\n",
       " Grotto,\n",
       " Marian,\n",
       " Lourdes,\n",
       " France,\n",
       " the Virgin Mary,\n",
       " Saint Bernadette Soubirous,\n",
       " 1858,\n",
       " 3,\n",
       " the Gold Dome,\n",
       " Mary)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "spacy_map[pairs[0][0]].ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 200 examples\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    12 (12.0%)\n",
      "\n",
      "Example fails:\n",
      "Q: Where have the powers maintained peace in recent years?\n",
      "P: United Nations\n",
      "\n",
      "Q: Where have the powers maintained peace inr ecent years?\n",
      "P: United Nations and other forums of international discussion\n",
      "\n",
      "\n",
      "----\n",
      "Q: Where in Houston is the University of Houston campus located?\n",
      "P: southeast\n",
      "\n",
      "Q: Where in Houston i sthe University of Houston campus located?\n",
      "P: southeast Houston\n",
      "\n",
      "\n",
      "----\n",
      "Q: Who is the only one who has the authority to stop the game when something is wron?\n",
      "P: referee\n",
      "\n",
      "Q: Who is the only one who has the authority to stop the game when somethign is wron?\n",
      "P: the referee\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Definition\n",
    "def question_typo(x):\n",
    "    \"\"\"\n",
    "    x[0]: context\n",
    "    x[1]: question \n",
    "    Perturb.add_typos(x[1]): add a typo to question \n",
    "    \"\"\"\n",
    "    return (x[0], Perturb.add_typos(x[1]))\n",
    "\n",
    "t = Perturb.perturb(pairs, question_typo, nsamples=500) # perturb is dataset to evaluate the robustness of BERT\n",
    "test = INV(**t, name='Question typo', capability='Robustness', description='')\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad) #format output\n",
    "suite.add(test, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 200 examples\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    7 (7.0%)\n",
      "\n",
      "Example fails:\n",
      "Q: What are some of the most widely known poultry tournaments?\n",
      "P: national and regional poultry shows\n",
      "\n",
      "Q: What're some of the most widely known poultry tournaments?\n",
      "P: National Championship Show\n",
      "\n",
      "\n",
      "----\n",
      "Q: Where did Japanese warriors come to literary maturity?\n",
      "P: the Heike Monogatari\n",
      "\n",
      "Q: Where'd Japanese warriors come to literary maturity?\n",
      "P: Heike Monogatari\n",
      "\n",
      "\n",
      "----\n",
      "Q: Who is Martha Ann Ricks?\n",
      "P: One of the most well-known Liberian quilters\n",
      "\n",
      "Q: Who's Martha Ann Ricks?\n",
      "P: Ellen Johnson Sirleaf\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# add the question contraction\n",
    "def contractions(x):\n",
    "    conts = Perturb.contractions(x[1])\n",
    "    return [(x[0], a) for a in conts]\n",
    "t = Perturb.perturb(pairs, contractions, nsamples=500)\n",
    "test = INV(**t, name='Question contractions', capability='Robustness', description='')\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add random sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add random sentence\n",
    "random_sentences = set()\n",
    "\n",
    "# generate the random sentence and saved as list\n",
    "for x, _ in processed_pairs:\n",
    "    for y in x.sents:\n",
    "        random_sentences.add(y.text)\n",
    "random_sentences = list(random_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is in front of the Notre Dame Main Building?\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "for y in spacy_map[pairs[1][1]].sents:\n",
    "    print(y.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92328"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 300 examples\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    16 (16.0%)\n",
      "\n",
      "Example fails:\n",
      "Q: What did the Chinese media focus on as far as human rights protesters?\n",
      "P: disruptive protesters\n",
      "\n",
      "Q: What did the Chinese media focus on as far as human rights protesters?\n",
      "P: more disruptive protesters\n",
      "Perturb: add to beg: In 1992, Gomes directed Udju Azul di Yonta, which was screened in the Un Certain Regard section at the 1992 Cannes Film Festival. \n",
      "\n",
      "\n",
      "----\n",
      "Q: What began in 1254 \n",
      "P: The construction of the present Gothic building was begun in 1254\n",
      "\n",
      "Q: What began in 1254 \n",
      "P: The construction of the present Gothic building\n",
      "Perturb: add to beg: The term Muslim world, also known as Islamic world and the Ummah (Arabic: أمة‎, meaning \"nation\" or \"community\") has different meanings. \n",
      "\n",
      "\n",
      "----\n",
      "Q: John remains a recurring character within what culture?\n",
      "P: Western\n",
      "\n",
      "Q: John remains a recurring character within what culture?\n",
      "P: Western popular culture\n",
      "Perturb: add to beg: Downtown Boston's streets grew organically, so they do not form a planned grid, unlike those in later-developed Back Bay, East Boston, the South End, and South Boston. \n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# function to add the random sentence at the beginning or ending\n",
    "def add_random_sentence(x, **kwargs):\n",
    "    random_s = np.random.choice(random_sentences)\n",
    "    while random_s in x[0]:\n",
    "        random_s = np.random.choice(random_sentences)\n",
    "    random_s = random_s.strip('.') + '. '\n",
    "    meta = ['add to end: %s' % random_s, 'add to beg: %s' % random_s]\n",
    "    return [(x[0] + random_s, x[1]), (random_s + x[0], x[1])], meta\n",
    "\n",
    "# format the result of perturb (add random sentence)\n",
    "def format_add(x, pred, conf, label=None, meta=None):\n",
    "    ret = format_squad(x, pred, conf, label, meta)\n",
    "    if meta:\n",
    "        ret += 'Perturb: %s\\n' % meta\n",
    "    return ret\n",
    "\n",
    "# Test the random sentence\n",
    "t = Perturb.perturb(pairs, add_random_sentence, nsamples=500, meta=True)\n",
    "test = INV(**t, name='Add random sentence to context', capability='Robustness', description='')\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_add)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# change the context  and question\n",
    "def change_thing(change_fn):\n",
    "    def change_both(cq, **kwargs):\n",
    "        context, question = cq\n",
    "        a = change_fn(context, meta=True)\n",
    "        if not a:\n",
    "            return None\n",
    "        changed, meta = a\n",
    "        ret = []\n",
    "        for c, m in zip(changed, meta):\n",
    "            new_q = re.sub(r'\\b%s\\b' % re.escape(m[0]), m[1], question.text) #new\n",
    "            ret.append((c, new_q))\n",
    "        return ret, meta\n",
    "    return change_both\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These function formatted the evaluation results\n",
    "def expect_same(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    if not meta:\n",
    "        return pred == orig_pred\n",
    "    return pred == re.sub(r'\\b%s\\b' % re.escape(meta[0]), meta[1], orig_pred)\n",
    "\n",
    "def format_replace(x, pred, conf, label=None, meta=None):\n",
    "    ret = format_squad(x, pred, conf, label, meta)\n",
    "    if meta:\n",
    "        ret += 'Perturb: %s -> %s\\n' % meta\n",
    "    return ret\n",
    "\n",
    "def format_replace_context(x, pred, conf, label=None, meta=None):\n",
    "    ret = format_squad_with_context(x, pred, conf, label, meta)\n",
    "    if meta:\n",
    "        ret += 'Perturb: %s -> %s\\n' % meta\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1100 examples\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    1 (1.0%)\n",
      "\n",
      "Example fails:\n",
      "Q: What was the subject under consideration in discussions in 1980?\n",
      "P: patriation of the Canadian constitution\n",
      "\n",
      "Q: What was the subject under consideration in discussions in 1980?\n",
      "P: Canadian constitution\n",
      "Perturb: Queen -> Alicia\n",
      "\n",
      "Q: What was the subject under consideration in discussions in 1980?\n",
      "P: the patriation of the Canadian constitution\n",
      "Perturb: Pierre Trudeau -> William Jones\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# change the name and add the replace word. \n",
    "# append the test case Perturb labed the formatted like `Perturb: %s -> %s`\n",
    "t = Perturb.perturb(processed_pairs, change_thing(Perturb.change_names), nsamples=500, meta=True)\n",
    "\n",
    "test = INV(**t, name='Change name everywhere', capability='NER',\n",
    "          description='', expect=Expect.pairwise(expect_same))\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(3, format_example_fn=format_replace)\n",
    "suite.add(test, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1100 examples\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    4 (4.0%)\n",
      "\n",
      "Example fails:\n",
      "Q: What roles was George Warren Brown known?\n",
      "P: a St. Louis philanthropist and co-founder of the Brown Shoe Company\n",
      "\n",
      "Q: What roles was George Warren Brown known?\n",
      "P: philanthropist and co-founder of the Brown Shoe Company\n",
      "Perturb: St. Louis -> Lexington-Fayette\n",
      "\n",
      "Q: What roles was George Warren Brown known?\n",
      "P: San Diego philanthropist and co-founder of the Brown Shoe Company\n",
      "Perturb: St. Louis -> San Diego\n",
      "\n",
      "\n",
      "----\n",
      "Q: What are some of the target countries?\n",
      "P: Bangladesh, Brazil, China, Egypt, India, Indonesia, Mexico, Nigeria\n",
      "\n",
      "Q: What are some of the target countries?\n",
      "P: Bangladesh, Brazil, Japan, Egypt\n",
      "Perturb: China -> Japan\n",
      "\n",
      "Q: What are some of the target countries?\n",
      "P: Bangladesh, Brazil, China, Egypt\n",
      "Perturb: Nigeria -> France\n",
      "\n",
      "\n",
      "----\n",
      "Q: What is the 3rd oldest paper in the nation?\n",
      "P: Philadelphia Inquirer\n",
      "\n",
      "Q: What is the 3rd oldest paper in the nation?\n",
      "P: The Anaheim Inquirer\n",
      "Perturb: Philadelphia -> Anaheim\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# change the location name and add the replace word. \n",
    "# append the test case Perturb labed the formatted like `Perturb: %s -> %s`\n",
    "t = Perturb.perturb(processed_pairs, change_thing(Perturb.change_location), nsamples=500, meta=True)\n",
    "\n",
    "test = INV(**t, name='Change location everywhere', capability='NER',\n",
    "          description='', expect=Expect.pairwise(expect_same))\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(3, format_example_fn=format_replace)\n",
    "suite.add(test, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 200 examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      480\n",
      "Test cases run:  100\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Test case on the Temporal\n",
    "# Mapping use the crossproduct for the test case.\n",
    "# change the profession in the second sentence\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            'Both {first_name} and {first_name2} were {prof1}s, but there was a change in {first_name}, who is now {a:prof2}.',\n",
    "            'Both {first_name2} and {first_name} were {prof1}s, but there was a change in {first_name}, who is now {a:prof2}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who is {a:prof2}?',\n",
    "                '{first_name}'\n",
    "            ), \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    save=True,\n",
    "    prof=professions,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    ))\n",
    "name = 'There was a change in profession'\n",
    "test = MFT(**t, expect=expect_squad, capability='Temporal', name=name, description='' )\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n",
      "Test cases:      497\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Laura became a waitress before Roy did.\n",
      "Q: Who became a waitress last?\n",
      "A: Roy\n",
      "P: Laura\n",
      "\n",
      "C: Roy became a waitress after Laura did.\n",
      "Q: Who became a waitress first?\n",
      "A: Laura\n",
      "P: Roy\n",
      "\n",
      "\n",
      "----\n",
      "C: Paul became a editor before Alexandra did.\n",
      "Q: Who became a editor last?\n",
      "A: Alexandra\n",
      "P: Paul\n",
      "\n",
      "C: Alexandra became a editor after Paul did.\n",
      "Q: Who became a editor first?\n",
      "A: Paul\n",
      "P: Alexandra\n",
      "\n",
      "\n",
      "----\n",
      "C: Daniel became a agent before Bobby did.\n",
      "Q: Who became a agent last?\n",
      "A: Bobby\n",
      "P: Daniel\n",
      "\n",
      "C: Bobby became a agent after Daniel did.\n",
      "Q: Who became a agent first?\n",
      "A: Daniel\n",
      "P: Bobby\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Testing on the before/after changed to first/last that understanding or not\n",
    "\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} became a {prof} before {first_name2} did.',\n",
    "            '{first_name2} became a {prof} after {first_name} did.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who became a {prof} first?',\n",
    "                '{first_name}'\n",
    "            ), \n",
    "            (\n",
    "                'Who became a {prof} last?',\n",
    "                '{first_name2}'\n",
    "            ), \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    save=True,\n",
    "    prof=professions,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    ))\n",
    "name = 'Understanding before / after -> first / last.'\n",
    "test = MFT(**t, expect=expect_squad, capability='Temporal', name=name, description='' )\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n",
      "Test cases:      497\n",
      "Test cases run:  100\n",
      "Fails (rate):    94 (94.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Mike is not an attorney. Tim is.\n",
      "Q: Who is an attorney?\n",
      "A: Tim\n",
      "P: Mike\n",
      "\n",
      "C: Tim is an attorney. Mike is not.\n",
      "Q: Who is not an attorney?\n",
      "A: Mike\n",
      "P: Tim\n",
      "\n",
      "\n",
      "----\n",
      "C: Michelle is not a historian. Ed is.\n",
      "Q: Who is a historian?\n",
      "A: Ed\n",
      "P: Michelle\n",
      "\n",
      "\n",
      "----\n",
      "C: Steven is not a waitress. Joan is.\n",
      "Q: Who is a waitress?\n",
      "A: Joan\n",
      "P: Steven\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Test case on the Negation\n",
    "# Mapping use the crossproduct for the test case.\n",
    "# Add the negation (not) in the context.\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} is not {a:prof}. {first_name2} is.',\n",
    "            '{first_name2} is {a:prof}. {first_name} is not.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who is {a:prof}?',\n",
    "                '{first_name2}'\n",
    "            ), \n",
    "            (\n",
    "                'Who is not {a:prof}?',\n",
    "                '{first_name}'\n",
    "            ), \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    save=True,\n",
    "    prof=professions,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    ))\n",
    "name = 'Negation in context, may or may not be in question'\n",
    "test = MFT(**t, expect=expect_squad, capability='Negation', name=name, description='' )\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not in context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 800 examples\n",
      "Test cases:      479\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Anne is a producer. Suzanne is an interpreter.\n",
      "Q: Who is not a producer?\n",
      "A: Suzanne\n",
      "P: Anne\n",
      "\n",
      "C: Anne is a producer. Suzanne is an interpreter.\n",
      "Q: Who is not an interpreter?\n",
      "A: Anne\n",
      "P: Suzanne\n",
      "\n",
      "C: Suzanne is an interpreter. Anne is a producer.\n",
      "Q: Who is not a producer?\n",
      "A: Suzanne\n",
      "P: Anne\n",
      "\n",
      "\n",
      "----\n",
      "C: Florence is a producer. Annie is an economist.\n",
      "Q: Who is not a producer?\n",
      "A: Annie\n",
      "P: Florence\n",
      "\n",
      "C: Florence is a producer. Annie is an economist.\n",
      "Q: Who is not an economist?\n",
      "A: Florence\n",
      "P: Annie\n",
      "\n",
      "C: Annie is an economist. Florence is a producer.\n",
      "Q: Who is not a producer?\n",
      "A: Annie\n",
      "P: Florence\n",
      "\n",
      "\n",
      "----\n",
      "C: Carol is a journalist. Alice is an engineer.\n",
      "Q: Who is not a journalist?\n",
      "A: Alice\n",
      "P: Carol\n",
      "\n",
      "C: Carol is a journalist. Alice is an engineer.\n",
      "Q: Who is not an engineer?\n",
      "A: Carol\n",
      "P: Alice\n",
      "\n",
      "C: Alice is an engineer. Carol is a journalist.\n",
      "Q: Who is not a journalist?\n",
      "A: Alice\n",
      "P: Carol\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Test case on the Negation\n",
    "# Mapping use the crossproduct for the test case.\n",
    "# Add the negation (not) in the question.\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} is {a:prof}. {first_name2} is {a:prof2}.',\n",
    "            '{first_name2} is {a:prof2}. {first_name} is {a:prof}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who is {a:prof}?',\n",
    "                '{first_name}'\n",
    "            ), \n",
    "            (\n",
    "                'Who is not {a:prof}?',\n",
    "                '{first_name2}'\n",
    "            ), \n",
    "            (\n",
    "                'Who is {a:prof2}?',\n",
    "                '{first_name2}'\n",
    "            ), \n",
    "            (\n",
    "                'Who is not {a:prof2}?',\n",
    "                '{first_name}'\n",
    "            ), \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    prof=professions,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    ))\n",
    "name = 'Negation in question only.'\n",
    "test = MFT(**t, expect=expect_squad, capability='Negation', name=name, description='' )\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness spinoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 100 examples\n",
      "profession    fail_men fail_women (count)\n",
      "doctor          5.7      5.7     (247)\n",
      "secretary       4.0      4.8     (249)\n",
      "CEO             4.0      4.0     (248)\n",
      "nurse           2.7      3.5     (256)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# Test on the fairness\n",
    "# add negation in the context\n",
    "fewer_profs = ['doctor', 'nurse', 'secretary', 'CEO']\n",
    "t = editor.template(\n",
    "    [\n",
    "        ('{male} is not {a:prof}, {female} is.', 'Who is {a:prof}?', '{female}', 'woman', '{prof}'),\n",
    "        ('{female} is not {a:prof}, {male} is.', 'Who is {a:prof}?', '{male}', 'man', '{prof}'),\n",
    "    ],\n",
    "#     prof=professions + ['doctor'],\n",
    "    prof=fewer_profs,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=1000,\n",
    "    unroll=True,\n",
    "    save=True,\n",
    "    )\n",
    "data = [(d[0], d[1]) for d in t.data]\n",
    "labels = [d[2] for d in t.data]\n",
    "meta = [(d[3], d[4]) for d in t.data]\n",
    "\n",
    "test = MFT(data, expect=expect_squad, labels=labels, meta=meta, templates=t.templates,\n",
    "          name='M/F failure rates should be similar for different professions', capability='Fairness',\n",
    "          description='Using negation in context.')\n",
    "test.run(predconfs, n=100)\n",
    "\n",
    "#function to formatted the fair output\n",
    "def print_fair(test):\n",
    "    c = collections.Counter(test.meta)\n",
    "    fail = collections.Counter([tuple(x) for x in np.array(test.meta)[test.fail_idxs()]])\n",
    "    profs = set()\n",
    "    for sex, prof in fail:\n",
    "        profs.add(prof)\n",
    "    prof_fail = {}\n",
    "    get_fail = lambda f:fail[f] / c[f]\n",
    "    for prof in profs:\n",
    "        fail_m = get_fail(('man', prof))\n",
    "        fail_f = get_fail(('woman', prof))\n",
    "        prof_fail[prof] = (fail_m, fail_f)\n",
    "    print('%-13s fail_men fail_women (count)' % 'profession')\n",
    "    for prof, vs in sorted(prof_fail.items(), key=lambda x:max(x[1][0], x[1][1]), reverse=True):\n",
    "        fail_m, fail_f = vs\n",
    "        print('%-13s   %.1f      %.1f     (%d)' % (prof, 100 * fail_m, 100 * fail_f, c[('man', prof)]))\n",
    "print_fair(test)\n",
    "\n",
    "# add test to the suite\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 800 examples\n",
      "Test cases:      489\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Richard and Emma are friends. He is an organizer, and she is an administrator.\n",
      "Q: Who is an organizer?\n",
      "A: Richard\n",
      "P: Richard and Emma\n",
      "\n",
      "C: Richard and Emma are friends. He is an organizer, and she is an administrator.\n",
      "Q: Who is an administrator?\n",
      "A: Emma\n",
      "P: Richard and Emma\n",
      "\n",
      "C: Emma and Richard are friends. He is an organizer, and she is an administrator.\n",
      "Q: Who is an organizer?\n",
      "A: Richard\n",
      "P: Emma and Richard\n",
      "\n",
      "\n",
      "----\n",
      "C: Roy and Nancy are friends. He is an economist, and she is a musician.\n",
      "Q: Who is an economist?\n",
      "A: Roy\n",
      "P: Roy and Nancy\n",
      "\n",
      "C: Roy and Nancy are friends. He is an economist, and she is a musician.\n",
      "Q: Who is a musician?\n",
      "A: Nancy\n",
      "P: Roy and Nancy\n",
      "\n",
      "C: Nancy and Roy are friends. He is an economist, and she is a musician.\n",
      "Q: Who is an economist?\n",
      "A: Roy\n",
      "P: Nancy and Roy\n",
      "\n",
      "\n",
      "----\n",
      "C: Larry and Jane are friends. He is a secretary, and she is an escort.\n",
      "Q: Who is a secretary?\n",
      "A: Larry\n",
      "P: Larry and Jane\n",
      "\n",
      "C: Larry and Jane are friends. He is a secretary, and she is an escort.\n",
      "Q: Who is an escort?\n",
      "A: Jane\n",
      "P: Larry and Jane\n",
      "\n",
      "C: Jane and Larry are friends. He is a secretary, and she is an escort.\n",
      "Q: Who is a secretary?\n",
      "A: Larry\n",
      "P: Jane and Larry\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# This test case test on the coref 'he and she'\n",
    "# In the context provide male and female are friends. Then Tell that He is {first_job}, and she is {second_job}.\n",
    "# After that asking who is the {first_job}. So that this test is to test he and she can be understandable by the system or not.\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{male} and {female} are friends. He is {a:prof1}, and she is {a:prof2}.',\n",
    "            '{female} and {male} are friends. He is {a:prof1}, and she is {a:prof2}.',\n",
    "            '{male} and {female} are friends. She is {a:prof2}, and he is {a:prof1}.',\n",
    "            '{female} and {male} are friends. She is {a:prof2}, and he is {a:prof1}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who is {a:prof1}?',\n",
    "                '{male}'\n",
    "            ), \n",
    "            (\n",
    "                'Who is {a:prof2}?',\n",
    "                '{female}'\n",
    "            ), \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    save=True,\n",
    "    prof=professions,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    ))\n",
    "name = 'Basic coref, he / she'\n",
    "test = MFT(**t, expect=expect_squad, name=name, description='', capability='Coref')\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 200 examples\n",
      "Test cases:      500\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Patrick and Charlotte are friends. His mom is an accountant.\n",
      "Q: Whose mom is an accountant?\n",
      "A: Patrick\n",
      "P: Patrick and Charlotte\n",
      "\n",
      "C: Charlotte and Patrick are friends. His mom is an accountant.\n",
      "Q: Whose mom is an accountant?\n",
      "A: Patrick\n",
      "P: Charlotte and Patrick\n",
      "\n",
      "\n",
      "----\n",
      "C: Ralph and Ann are friends. His mom is an entrepreneur.\n",
      "Q: Whose mom is an entrepreneur?\n",
      "A: Ralph\n",
      "P: Ralph and Ann\n",
      "\n",
      "C: Ann and Ralph are friends. His mom is an entrepreneur.\n",
      "Q: Whose mom is an entrepreneur?\n",
      "A: Ralph\n",
      "P: Ann and Ralph\n",
      "\n",
      "\n",
      "----\n",
      "C: Ralph and Pamela are friends. Her mom is a waitress.\n",
      "Q: Whose mom is a waitress?\n",
      "A: Pamela\n",
      "P: Ralph and Pamela\n",
      "\n",
      "C: Pamela and Ralph are friends. Her mom is a waitress.\n",
      "Q: Whose mom is a waitress?\n",
      "A: Pamela\n",
      "P: Pamela and Ralph\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Test on the basic his/her \n",
    "# The example failed:\n",
    "# C: Patrick and Charlotte are friends. His mom is an accountant.\n",
    "# Q: Whose mom is an accountant?\n",
    "# A: Patrick\n",
    "# P: Patrick and Charlotte\n",
    "# In my opinion, if it added the chain-of-thought, the failer rate will be decreased.\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{male} and {female} are friends. His mom is {a:prof}.',\n",
    "            '{female} and {male} are friends. His mom is {a:prof}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Whose mom is {a:prof}?',\n",
    "                '{male}'\n",
    "            ), \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    save=True,\n",
    "    prof=professions,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=250,\n",
    "    ))\n",
    "t += crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{male} and {female} are friends. Her mom is {a:prof}.',\n",
    "            '{female} and {male} are friends. Her mom is {a:prof}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Whose mom is {a:prof}?',\n",
    "                '{female}'\n",
    "            ), \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    save=True,\n",
    "    prof=professions,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=250,\n",
    "    ))\n",
    "\n",
    "name = 'Basic coref, his / her'\n",
    "test = MFT(**t, expect=expect_squad, name=name, description='', capability='Coref')\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Former, latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n",
      "Test cases:      481\n",
      "Test cases run:  100\n",
      "Fails (rate):    99 (99.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Sarah and Harriet are friends. The former is an agent.\n",
      "Q: Who is an agent?\n",
      "A: Sarah\n",
      "P: Sarah and Harriet\n",
      "\n",
      "C: Harriet and Sarah are friends. The latter is an agent.\n",
      "Q: Who is an agent?\n",
      "A: Sarah\n",
      "P: Harriet and Sarah\n",
      "\n",
      "C: Sarah and Harriet are friends. The former is an agent and the latter is an economist.\n",
      "Q: Who is an agent?\n",
      "A: Sarah\n",
      "P: Sarah and Harriet\n",
      "\n",
      "\n",
      "----\n",
      "C: Simon and Michael are friends. The former is an assistant.\n",
      "Q: Who is an assistant?\n",
      "A: Simon\n",
      "P: Simon and Michael\n",
      "\n",
      "C: Michael and Simon are friends. The latter is an assistant.\n",
      "Q: Who is an assistant?\n",
      "A: Simon\n",
      "P: Michael and Simon\n",
      "\n",
      "C: Simon and Michael are friends. The former is an assistant and the latter is an entrepreneur.\n",
      "Q: Who is an assistant?\n",
      "A: Simon\n",
      "P: Simon and Michael\n",
      "\n",
      "\n",
      "----\n",
      "C: Sharon and Linda are friends. The former is an attorney.\n",
      "Q: Who is an attorney?\n",
      "A: Sharon\n",
      "P: Sharon and Linda\n",
      "\n",
      "C: Linda and Sharon are friends. The latter is an attorney.\n",
      "Q: Who is an attorney?\n",
      "A: Sharon\n",
      "P: Linda and Sharon\n",
      "\n",
      "C: Sharon and Linda are friends. The former is an attorney and the latter is an organizer.\n",
      "Q: Who is an attorney?\n",
      "A: Sharon\n",
      "P: Sharon and Linda\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# This test cases is for test the Former(before) and Latter(after)\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} and {first_name2} are friends. The former is {a:prof1}.',\n",
    "            '{first_name2} and {first_name} are friends. The latter is {a:prof1}.',\n",
    "            '{first_name} and {first_name2} are friends. The former is {a:prof1} and the latter is {a:prof2}.',\n",
    "            '{first_name2} and {first_name} are friends. The former is {a:prof2} and the latter is {a:prof1}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who is {a:prof1}?',\n",
    "                '{first_name}'\n",
    "            ), \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    prof=professions,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    save=True\n",
    "    ))\n",
    "name = 'Former / Latter'\n",
    "test = MFT(**t, expect=expect_squad, name=name, description='', capability='Coref')\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 400 examples\n",
      "Test cases:      497\n",
      "Test cases run:  100\n",
      "Fails (rate):    68 (68.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Mary hates Robin.\n",
      "Q: Who is hated?\n",
      "A: Robin\n",
      "P: Mary\n",
      "\n",
      "\n",
      "----\n",
      "C: Philip deserves Florence.\n",
      "Q: Who is deserved?\n",
      "A: Florence\n",
      "P: Philip deserves Florence\n",
      "\n",
      "\n",
      "----\n",
      "C: Mary bothers Paul.\n",
      "Q: Who bothers?\n",
      "A: Mary\n",
      "P: Paul\n",
      "\n",
      "C: Paul is bothered by Mary.\n",
      "Q: Who bothers?\n",
      "A: Mary\n",
      "P: Paul\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import pattern\n",
    "import pattern.en\n",
    "pverb = ['love', 'hate', 'like', 'remember', 'recognize', 'trust', 'deserve', 'understand', 'blame', 'dislike', 'prefer', 'follow', 'notice', 'hurt', 'bother', 'support', 'believe', 'accept', 'attack']\n",
    "a = pattern.en.tenses('loves')[0]\n",
    "b = pattern.en.tenses('stolen')[0]\n",
    "pverb = [(pattern.en.conjugate(v, *a), pattern.en.conjugate(v, *b)) for v in pverb]\n",
    "\n",
    "# Test the object distinction. In the context and question are swap when testing. If context is active voice, the question will be the passive voice. \n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} {v[0]} {first_name2}.',\n",
    "            '{first_name2} is {v[1]} by {first_name}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who {v[0]}?',\n",
    "                '{first_name}'\n",
    "            ), \n",
    "            (\n",
    "                'Who is {v[1]}?',\n",
    "                '{first_name2}'\n",
    "            ), \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    v=pverb,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    ))\n",
    "name = 'Agent / object distinction'\n",
    "test = MFT(**t, expect=expect_squad, name=name, description='', capability='SRL')\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1600 examples\n",
      "Test cases:      496\n",
      "Test cases run:  100\n",
      "Fails (rate):    100 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Bob remembers Steve. Sarah is remembered by Steve.\n",
      "Q: Who remembers Sarah?\n",
      "A: Steve\n",
      "P: Bob\n",
      "\n",
      "C: Bob remembers Steve. Sarah is remembered by Steve.\n",
      "Q: Who is remembered by Bob?\n",
      "A: Steve\n",
      "P: Sarah\n",
      "\n",
      "C: Steve is remembered by Bob. Sarah is remembered by Steve.\n",
      "Q: Who is remembered by Bob?\n",
      "A: Steve\n",
      "P: Sarah\n",
      "\n",
      "\n",
      "----\n",
      "C: Rachel hates Amanda. Amanda hates Roy.\n",
      "Q: Who is hated by Rachel?\n",
      "A: Amanda\n",
      "P: Roy\n",
      "\n",
      "C: Rachel hates Amanda. Roy is hated by Amanda.\n",
      "Q: Who hates Roy?\n",
      "A: Amanda\n",
      "P: Rachel\n",
      "\n",
      "C: Rachel hates Amanda. Roy is hated by Amanda.\n",
      "Q: Who is hated by Rachel?\n",
      "A: Amanda\n",
      "P: Roy\n",
      "\n",
      "\n",
      "----\n",
      "C: Larry loves Melissa. Bobby is loved by Melissa.\n",
      "Q: Who loves Bobby?\n",
      "A: Melissa\n",
      "P: Larry\n",
      "\n",
      "C: Larry loves Melissa. Bobby is loved by Melissa.\n",
      "Q: Who is loved by Larry?\n",
      "A: Melissa\n",
      "P: Bobby\n",
      "\n",
      "C: Melissa is loved by Larry. Melissa loves Bobby.\n",
      "Q: Who is loved by Melissa?\n",
      "A: Bobby\n",
      "P: Larry\n",
      "\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# To test the object distinction between 3\n",
    "# in the context has 2 sentences. If first sentence is active voice, the second sentence will be passive voice. \n",
    "# In the question ask with the different formatted. The test is not understand the meaning. It us just only mask the context and question, but if we ask with \n",
    "# another order of sentence, the predicted answer will always answer with the wrong answer for the test case (distinction between 3)\n",
    "t = crossproduct(editor.template(\n",
    "    {\n",
    "        'contexts': [\n",
    "            '{first_name} {v[0]} {first_name2}. {first_name2} {v[0]} {first_name3}.',\n",
    "            '{first_name} {v[0]} {first_name2}. {first_name3} is {v[1]} by {first_name2}.',\n",
    "            '{first_name2} is {v[1]} by {first_name}. {first_name2} {v[0]} {first_name3}.',\n",
    "            '{first_name2} is {v[1]} by {first_name}. {first_name3} is {v[1]} by {first_name2}.',\n",
    "        ],\n",
    "        'qas': [\n",
    "            (\n",
    "                'Who {v[0]} {first_name2}?',\n",
    "                '{first_name}'\n",
    "            ), \n",
    "            (\n",
    "                'Who {v[0]} {first_name3}?',\n",
    "                '{first_name2}'\n",
    "            ), \n",
    "            (\n",
    "                'Who is {v[1]} by {first_name}?',\n",
    "                '{first_name2}'\n",
    "            ), \n",
    "            (\n",
    "                'Who is {v[1]} by {first_name2}?',\n",
    "                '{first_name3}'\n",
    "            ), \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    save=True,\n",
    "    v=pverb,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=500,\n",
    "    ))\n",
    "name = 'Agent / object distinction with 3 agents'\n",
    "test = MFT(**t, expect=expect_squad, name=name, description='', capability='SRL')\n",
    "test.run(predconfs, n=100)\n",
    "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
    "suite.add(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'squad_suite.pkl' #define path\n",
    "suite.save(path) #save the test case into path that define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
